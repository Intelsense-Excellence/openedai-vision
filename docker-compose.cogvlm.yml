services:
  server:
    build:
      dockerfile: Dockerfile.cogvlm
    tty: true
    image: ghcr.io/matatonic/cogvlm
    environment:
      - HF_HOME=/app/hf_home
      - MODEL_PATH=THUDM/cogagent-chat-hf # (40GB or 14GB quant)
      #- MODEL_PATH=THUDM/cogvlm-chat-hf # (40GB or 14GB quant)
      #- QUANT_ENABLED=1 # use 4bit, auto-enabled if device has < 40GB
      #- CUDA_VISIBLE_DEVICES=1,0
    volumes:
      - ./hf_home:/app/hf_home
    ports:
      - 5006:8000
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #device_ids: ['0', '1'] # Select a gpu, or
              count: all
              capabilities: [gpu]
