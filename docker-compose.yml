services:
  server:
    build:
      context: .
      dockerfile: Dockerfile
    tty: true
    image: ghcr.io/matatonic/openedai-vision
    environment:
      - HF_HOME=/app/hf_home
      #- CUDA_VISIBLE_DEVICES=1
    volumes:
      - ./hf_home:/app/hf_home  # for Hugginface model cache
    # be sure to review and run prepare_minigemini.sh before starting a mini-gemini model
      - ./model_zoo:/app/model_zoo # for MiniGemini
      - ./YanweiLi:/app/YanweiLi # for MiniGemini
    ports:
      - 5006:5006
    # If a flag option is not listed here it probably doesn't work.
    #command: ["python", "vision.py", "-m", "internlm/internlm-xcomposer2-7b", "--use-flash-attn", '-d', 'cuda:0']
    #command: ["python", "vision.py", "-m", "internlm/internlm-xcomposer2-7b-4bit", "--use-flash-attn"] # not recommended, bad quant.
    #command: ["python", "vision.py", "-m", "internlm/internlm-xcomposer2-vl-7b", "--use-flash-attn", "-d", "cuda:0"]
    #command: ["python", "vision.py", "-m", "internlm/internlm-xcomposer2-vl-7b-4bit", "--use-flash-attn"]
    #command: ["python", "vision.py", "-m", "llava-hf/llava-v1.6-34b-hf", "--load-in-4bit", "--use-flash-attn"] # very, very slow...
    #command: ["python", "vision.py", "-m", "echo840/Monkey-Chat"] 
    #command: ["python", "vision.py", "-m", "echo840/Monkey"]
    #command: ["python", "vision.py", "-m", "openbmb/OmniLMM-12B"] # WIP
    #command: ["python", "vision.py", "-m", "deepseek-ai/deepseek-vl-7b-chat"] # WIP
    #command: ["python", "vision.py", "-m", "llava-hf/llava-v1.6-mistral-7b-hf", "--load-in-4bit", "--use-flash-attn"]
    #command: ["python", "vision.py", "-m", "llava-hf/llava-v1.6-vicuna-7b-hf", "--load-in-4bit", "--use-flash-attn"]
    #command: ["python", "vision.py", "-m", "llava-hf/llava-v1.6-vicuna-13b-hf", "--load-in-4bit", "--use-flash-attn"]
    #command: ["python", "vision.py", "-m", "Qwen/Qwen-VL-Chat"]
    #command: ["python", "vision.py", "-m", "llava-hf/bakLlava-v1-hf"] #, "--load-in-4bit", "--use-flash-attn"] # broken
    #command: ["python", "vision.py", "-m", "llava-hf/llava-1.5-13b-hf", "--load-in-4bit", "--use-flash-attn"]
    #command: ["python", "vision.py", "-m", "openbmb/MiniCPM-V", "--use-flash-attn", '-d', 'cuda:0']
    #command: ["python", "vision.py", "-m", "llava-hf/llava-1.5-7b-hf", "--load-in-4bit", "--use-flash-attn"]
    command: ["python", "vision.py", "-m", "vikhyatk/moondream2", "--use-flash-attn"]
    #command: ["python", "vision.py", "-m", "vikhyatk/moondream1"] # broken
    #command: ["python", "vision.py", "-m", "YanweiLi/Mini-Gemini-2B"] # be sure to review and run prepare_minigemini.sh before starting minigemini
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #device_ids: ['0', '1'] # Select a gpu, or
              count: all
              capabilities: [gpu]
