[
  ["HuggingFaceM4/idefics2-8b", "-A", "flash_attention_2", "--device-map", "cuda:0", "--load-in-4bit"],
  ["HuggingFaceM4/idefics2-8b", "-A", "flash_attention_2", "--device-map", "cuda:0"],
  ["HuggingFaceM4/idefics2-8b-AWQ", "-A", "flash_attention_2", "--device-map", "cuda:0"],
  ["HuggingFaceM4/idefics2-8b-chatty", "-A", "flash_attention_2", "--device-map", "cuda:0", "--load-in-4bit"],
  ["HuggingFaceM4/idefics2-8b-chatty", "-A", "flash_attention_2", "--device-map", "cuda:0"],
  ["HuggingFaceM4/idefics2-8b-chatty-AWQ", "-A", "flash_attention_2", "--device-map", "cuda:0"],
  ["OpenGVLab/InternVL2-4B", "--device-map", "cuda:0", "--load-in-4bit"],
  ["OpenGVLab/InternVL2-4B", "--device-map", "cuda:0"],
  ["OpenGVLab/Mini-InternVL-Chat-4B-V1-5", "--max-tiles", "40", "--load-in-4bit"],
  ["OpenGVLab/Mini-InternVL-Chat-4B-V1-5", "--load-in-4bit"],
  ["OpenGVLab/Mini-InternVL-Chat-4B-V1-5"],
  ["THUDM/cogagent-chat-hf", "--load-in-4bit"],
  ["THUDM/cogagent-chat-hf"],
  ["THUDM/cogvlm-chat-hf", "--load-in-4bit"],
  ["THUDM/cogvlm-chat-hf"],
  ["THUDM/cogvlm2-llama3-chat-19B", "--load-in-4bit"],
  ["THUDM/cogvlm2-llama3-chat-19B"],
  ["THUDM/cogvlm2-llama3-chinese-chat-19B", "--load-in-4bit"],
  ["THUDM/cogvlm2-llama3-chinese-chat-19B"],
  ["cognitivecomputations/dolphin-vision-72b", "-A", "flash_attention_2", "--load-in-4bit", "--device-map", "cuda:0"],
  ["cognitivecomputations/dolphin-vision-7b", "-A", "flash_attention_2", "--load-in-4bit", "--device-map", "cuda:0"],
  ["cognitivecomputations/dolphin-vision-7b", "-A", "flash_attention_2", "--device-map", "cuda:0"],
  ["llava-hf/llava-v1.6-mistral-7b-hf", "-A", "flash_attention_2", "--load-in-4bit"],
  ["llava-hf/llava-v1.6-mistral-7b-hf", "-A", "flash_attention_2"],
  ["openbmb/MiniCPM-V", "-A", "flash_attention_2", "--device-map", "cuda:0", "--load-in-4bit"],
  ["openbmb/MiniCPM-V", "-A", "flash_attention_2", "--device-map", "cuda:0"],
  ["openbmb/MiniCPM-V-2", "-A", "flash_attention_2", "--device-map", "cuda:0", "--load-in-4bit"],
  ["openbmb/MiniCPM-V-2", "-A", "flash_attention_2", "--device-map", "cuda:0"],
  ["tiiuae/falcon-11B-vlm", "-A", "flash_attention_2", "--load-in-4bit"],
  ["tiiuae/falcon-11B-vlm", "-A", "flash_attention_2"]
]
